# --weight_calib_method AMAX_SYM --weight_dtype int8 --act_calib_method MINMAX_ASYM --act_dtype int8 --weight_nbits 8 --act_nbits 8 --weight_granularity channel --act_granularity channel --kv_dtype int8 --kv_granularity head --is_dynamic_quant False --disable_input True --disable_output false
major_dtype:
  weight_dtype: int8
  act_dtype: int8
  weight_granularity: channel
  act_granularity: channel
  kv_dtype: int8
  kv_granularity: head
  disable_input: true
  disable_output: false
quantized op list:
  transformer_wte:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.wte._input_quantizer
        input_dtype: torch.int32
        input_shape:
        - 1
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_weight:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.wte._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 50401
        - 4096
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  transformer_h_0_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_0_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_0_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_0_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_0_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_0_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_0_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.0.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_1_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_1_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_1_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_1_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_1_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_1_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_1_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.1.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_2_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_2_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_2_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_2_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_2_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_2_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_2_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.2.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_3_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_3_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_3_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_3_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_3_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_3_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_3_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.3.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_4_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_4_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_4_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_4_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_4_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_4_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_4_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.4.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_5_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_5_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_5_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_5_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_5_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_5_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_5_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.5.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_6_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_6_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_6_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_6_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_6_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_6_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_6_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.6.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_7_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_7_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_7_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_7_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_7_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_7_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_7_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.7.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_8_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_8_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_8_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_8_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_8_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_8_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_8_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.8.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_9_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_9_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_9_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_9_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_9_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_9_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_9_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.9.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_10_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_10_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_10_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_10_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_10_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_10_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_10_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.10.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_11_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_11_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_11_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_11_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_11_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_11_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_11_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.11.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_12_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_12_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_12_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_12_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_12_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_12_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_12_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.12.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_13_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_13_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_13_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_13_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_13_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_13_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_13_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.13.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_14_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_14_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_14_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_14_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_14_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_14_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_14_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.14.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_15_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_15_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_15_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_15_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_15_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_15_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_15_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.15.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_16_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_16_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_16_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_16_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_16_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_16_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_16_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.16.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_17_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_17_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_17_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_17_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_17_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_17_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_17_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.17.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_18_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_18_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_18_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_18_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_18_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_18_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_18_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.18.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_19_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_19_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_19_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_19_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_19_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_19_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_19_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.19.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_20_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_20_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_20_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_20_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_20_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_20_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_20_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.20.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_21_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_21_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_21_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_21_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_21_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_21_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_21_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.21.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_22_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_22_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_22_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_22_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_22_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_22_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_22_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.22.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_23_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_23_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_23_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_23_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_23_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_23_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_23_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.23.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_24_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_24_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_24_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_24_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_24_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_24_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_24_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.24.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_25_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_25_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_25_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_25_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_25_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_25_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_25_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.25.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_26_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_26_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_26_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_26_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_26_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_26_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_26_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.26.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_27_ln_1:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.ln_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  transformer_h_27_attn_q_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.attn.q_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.attn.q_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_27_attn_k_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.attn.k_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.attn.k_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_27_attn_v_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.attn.v_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.attn.v_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_27_attn_out_proj:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.attn.out_proj._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.attn.out_proj._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_27_mlp_fc_in:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: int8
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.mlp.fc_in._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.mlp.fc_in._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 16384
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: true
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_h_27_mlp_fc_out:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.mlp.fc_out._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.h.27.mlp.fc_out._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 4096
        - 16384
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  transformer_ln_f:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: transformer.ln_f._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  lm_head:
    output_shape:
    - 1
    - 2048
    - 50401
    quant_desc_input:
      dtype: int8
      axis: -1
      dynamic: false
      etc_for_MCLab:
        torch_name: lm_head._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_weight:
      dtype: int8
      axis: 0
      dynamic: false
      etc_for_MCLab:
        torch_name: lm_head._weight_quantizer
        input_dtype: torch.float32
        input_shape:
        - 50401
        - 4096
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: max
        calibrator_method: amax
        asymmetric: false
        do_quant: true
        do_zp_equalizing: false
  einsum:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_1:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_1._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_1._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_1:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_1._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_1._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_1:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_1._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_1._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_2:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_2._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_2._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_1:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_1._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_1._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_3:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_3._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_3._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_5:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_5._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_5._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_4:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_4._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_4._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_6:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_6._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_6._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_7:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_7._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_7._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_2:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_2._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_2._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_3:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_3._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_3._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_2:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_2._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_2._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_3:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_3._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_3._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_2:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_2._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_2._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_2:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_2._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_2._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_1:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_3:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_3._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_3._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_7:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_7._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_7._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_3:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_3._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_3._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_1:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_1._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_8:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_8._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_8._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_12:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_12._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_12._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_9:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_9._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_9._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_13:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_13._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_13._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_14:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_14._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_14._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_4:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_4._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_4._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_5:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_5._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_5._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_4:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_4._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_4._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_5:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_5._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_5._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_4:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_4._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_4._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_4:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_4._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_4._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_2:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_2._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_5:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_5._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_5._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_12:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_12._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_12._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_5:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_5._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_5._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_2:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_2._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_13:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_13._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_13._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_19:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_19._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_19._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_14:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_14._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_14._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_20:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_20._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_20._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_21:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_21._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_21._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_6:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_6._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_6._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_7:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_7._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_7._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_6:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_6._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_6._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_7:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_7._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_7._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_6:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_6._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_6._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_6:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_6._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_6._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_3:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_3._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_7:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_7._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_7._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_17:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_17._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_17._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_7:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_7._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_7._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_3:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_3._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_18:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_18._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_18._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_26:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_26._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_26._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_19:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_19._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_19._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_27:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_27._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_27._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_28:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_28._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_28._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_8:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_8._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_8._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_9:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_9._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_9._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_8:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_8._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_8._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_9:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_9._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_9._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_8:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_8._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_8._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_8:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_8._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_8._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_4:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_4._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_9:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_9._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_9._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_22:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_22._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_22._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_9:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_9._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_9._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_4:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_4._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_23:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_23._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_23._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_33:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_33._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_33._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_24:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_24._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_24._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_34:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_34._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_34._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_35:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_35._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_35._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_10:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_10._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_10._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_11:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_11._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_11._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_10:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_10._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_10._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_11:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_11._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_11._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_10:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_10._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_10._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_10:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_10._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_10._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_5:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_5._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_11:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_11._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_11._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_27:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_27._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_27._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_11:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_11._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_11._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_5:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_5._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_28:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_28._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_28._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_40:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_40._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_40._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_29:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_29._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_29._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_41:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_41._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_41._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_42:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_42._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_42._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_12:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_12._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_12._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_13:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_13._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_13._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_12:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_12._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_12._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_13:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_13._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_13._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_12:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_12._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_12._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_12:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_12._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_12._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_6:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_6._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_13:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_13._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_13._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_32:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_32._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_32._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_13:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_13._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_13._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_6:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_6._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_33:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_33._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_33._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_47:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_47._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_47._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_34:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_34._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_34._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_48:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_48._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_48._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_49:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_49._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_49._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_14:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_14._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_14._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_15:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_15._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_15._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_14:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_14._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_14._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_15:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_15._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_15._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_14:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_14._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_14._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_14:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_14._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_14._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_7:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_7._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_15:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_15._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_15._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_37:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_37._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_37._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_15:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_15._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_15._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_7:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_7._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_38:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_38._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_38._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_54:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_54._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_54._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_39:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_39._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_39._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_55:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_55._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_55._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_56:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_56._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_56._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_16:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_16._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_16._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_17:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_17._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_17._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_16:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_16._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_16._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_17:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_17._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_17._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_16:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_16._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_16._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_16:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_16._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_16._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_8:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_8._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_17:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_17._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_17._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_42:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_42._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_42._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_17:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_17._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_17._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_8:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_8._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_43:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_43._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_43._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_61:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_61._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_61._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_44:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_44._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_44._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_62:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_62._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_62._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_63:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_63._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_63._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_18:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_18._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_18._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_19:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_19._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_19._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_18:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_18._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_18._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_19:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_19._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_19._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_18:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_18._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_18._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_18:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_18._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_18._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_9:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_9._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_19:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_19._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_19._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_47:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_47._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_47._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_19:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_19._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_19._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_9:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_9._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_48:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_48._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_48._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_68:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_68._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_68._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_49:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_49._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_49._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_69:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_69._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_69._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_70:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_70._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_70._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_20:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_20._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_20._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_21:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_21._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_21._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_20:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_20._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_20._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_21:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_21._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_21._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_20:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_20._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_20._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_20:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_20._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_20._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_10:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_10._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_21:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_21._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_21._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_52:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_52._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_52._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_21:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_21._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_21._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_10:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_10._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_53:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_53._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_53._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_75:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_75._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_75._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_54:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_54._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_54._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_76:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_76._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_76._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_77:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_77._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_77._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_22:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_22._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_22._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_23:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_23._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_23._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_22:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_22._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_22._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_23:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_23._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_23._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_22:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_22._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_22._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_22:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_22._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_22._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_11:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_11._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_23:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_23._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_23._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_57:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_57._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_57._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_23:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_23._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_23._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_11:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_11._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_58:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_58._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_58._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_82:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_82._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_82._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_59:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_59._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_59._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_83:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_83._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_83._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_84:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_84._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_84._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_24:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_24._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_24._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_25:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_25._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_25._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_24:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_24._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_24._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_25:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_25._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_25._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_24:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_24._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_24._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_24:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_24._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_24._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_12:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_12._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_25:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_25._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_25._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_62:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_62._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_62._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_25:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_25._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_25._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_12:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_12._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_63:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_63._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_63._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_89:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_89._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_89._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_64:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_64._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_64._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_90:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_90._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_90._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_91:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_91._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_91._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_26:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_26._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_26._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_27:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_27._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_27._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_26:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_26._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_26._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_27:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_27._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_27._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_26:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_26._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_26._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_26:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_26._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_26._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_13:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_13._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_27:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_27._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_27._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_67:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_67._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_67._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_27:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_27._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_27._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_13:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_13._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_68:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_68._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_68._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_96:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_96._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_96._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_69:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_69._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_69._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_97:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_97._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_97._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_98:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_98._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_98._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_28:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_28._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_28._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_29:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_29._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_29._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_28:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_28._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_28._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_29:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_29._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_29._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_28:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_28._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_28._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_28:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_28._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_28._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_14:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_14._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_29:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_29._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_29._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_72:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_72._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_72._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_29:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_29._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_29._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_14:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_14._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_73:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_73._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_73._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_103:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_103._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_103._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_74:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_74._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_74._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_104:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_104._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_104._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_105:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_105._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_105._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_30:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_30._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_30._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_31:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_31._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_31._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_30:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_30._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_30._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_31:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_31._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_31._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_30:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_30._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_30._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_30:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_30._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_30._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_15:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_15._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_31:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_31._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_31._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_77:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_77._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_77._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_31:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_31._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_31._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_15:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_15._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_78:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_78._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_78._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_110:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_110._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_110._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_79:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_79._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_79._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_111:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_111._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_111._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_112:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_112._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_112._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_32:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_32._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_32._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_33:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_33._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_33._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_32:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_32._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_32._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_33:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_33._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_33._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_32:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_32._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_32._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_32:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_32._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_32._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_16:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_16._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_33:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_33._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_33._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_82:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_82._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_82._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_33:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_33._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_33._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_16:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_16._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_83:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_83._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_83._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_117:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_117._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_117._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_84:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_84._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_84._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_118:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_118._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_118._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_119:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_119._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_119._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_34:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_34._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_34._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_35:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_35._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_35._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_34:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_34._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_34._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_35:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_35._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_35._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_34:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_34._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_34._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_34:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_34._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_34._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_17:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_17._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_35:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_35._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_35._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_87:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_87._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_87._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_35:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_35._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_35._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_17:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_17._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_88:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_88._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_88._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_124:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_124._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_124._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_89:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_89._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_89._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_125:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_125._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_125._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_126:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_126._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_126._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_36:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_36._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_36._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_37:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_37._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_37._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_36:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_36._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_36._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_37:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_37._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_37._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_36:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_36._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_36._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_36:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_36._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_36._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_18:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_18._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_37:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_37._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_37._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_92:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_92._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_92._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_37:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_37._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_37._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_18:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_18._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_93:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_93._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_93._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_131:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_131._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_131._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_94:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_94._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_94._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_132:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_132._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_132._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_133:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_133._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_133._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_38:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_38._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_38._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_39:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_39._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_39._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_38:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_38._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_38._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_39:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_39._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_39._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_38:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_38._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_38._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_38:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_38._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_38._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_19:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_19._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_39:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_39._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_39._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_97:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_97._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_97._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_39:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_39._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_39._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_19:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_19._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_98:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_98._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_98._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_138:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_138._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_138._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_99:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_99._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_99._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_139:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_139._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_139._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_140:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_140._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_140._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_40:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_40._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_40._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_41:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_41._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_41._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_40:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_40._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_40._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_41:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_41._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_41._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_40:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_40._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_40._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_40:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_40._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_40._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_20:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_20._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_41:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_41._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_41._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_102:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_102._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_102._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_41:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_41._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_41._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_20:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_20._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_103:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_103._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_103._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_145:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_145._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_145._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_104:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_104._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_104._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_146:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_146._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_146._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_147:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_147._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_147._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_42:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_42._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_42._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_43:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_43._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_43._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_42:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_42._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_42._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_43:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_43._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_43._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_42:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_42._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_42._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_42:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_42._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_42._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_21:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_21._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_43:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_43._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_43._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_107:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_107._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_107._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_43:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_43._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_43._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_21:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_21._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_108:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_108._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_108._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_152:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_152._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_152._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_109:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_109._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_109._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_153:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_153._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_153._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_154:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_154._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_154._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_44:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_44._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_44._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_45:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_45._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_45._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_44:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_44._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_44._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_45:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_45._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_45._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_44:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_44._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_44._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_44:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_44._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_44._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_22:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_22._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_45:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_45._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_45._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_112:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_112._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_112._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_45:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_45._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_45._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_22:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_22._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_113:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_113._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_113._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_159:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_159._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_159._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_114:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_114._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_114._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_160:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_160._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_160._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_161:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_161._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_161._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_46:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_46._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_46._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_47:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_47._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_47._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_46:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_46._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_46._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_47:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_47._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_47._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_46:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_46._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_46._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_46:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_46._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_46._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_23:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_23._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_47:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_47._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_47._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_117:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_117._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_117._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_47:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_47._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_47._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_23:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_23._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_118:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_118._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_118._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_166:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_166._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_166._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_119:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_119._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_119._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_167:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_167._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_167._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_168:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_168._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_168._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_48:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_48._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_48._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_49:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_49._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_49._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_48:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_48._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_48._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_49:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_49._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_49._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_48:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_48._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_48._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_48:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_48._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_48._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_24:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_24._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_49:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_49._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_49._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_122:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_122._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_122._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_49:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_49._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_49._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_24:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_24._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_123:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_123._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_123._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_173:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_173._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_173._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_124:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_124._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_124._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_174:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_174._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_174._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_175:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_175._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_175._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_50:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_50._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_50._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_51:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_51._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_51._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_50:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_50._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_50._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_51:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_51._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_51._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_50:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_50._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_50._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_50:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_50._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_50._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_25:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_25._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_51:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_51._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_51._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_127:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_127._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_127._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_51:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_51._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_51._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_25:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_25._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_128:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_128._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_128._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_180:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_180._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_180._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_129:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_129._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_129._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_181:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_181._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_181._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_182:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_182._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_182._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_52:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_52._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_52._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_53:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_53._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_53._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_52:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_52._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_52._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_53:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_53._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_53._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_52:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_52._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_52._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_52:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_52._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_52._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_26:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_26._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_53:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_53._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_53._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_132:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_132._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_132._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_53:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_53._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_53._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_26:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_26._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_133:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_133._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_133._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_187:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_187._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_187._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_134:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_134._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_134._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_188:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_188._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_188._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_189:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_189._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_189._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_54:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_54._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_54._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  einsum_55:
    output_shape:
    - 1
    - 2048
    - 16
    - 32
    - 2
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_55._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 32
        - 2
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_2:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: einsum_55._input_2_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 32
        - 2
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_54:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_54._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_54._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  cat_55:
    output_shape:
    - 1
    - 2048
    - 16
    - 256
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_55._input_quantizer.0
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 64
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: cat_55._input_quantizer.1
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16
        - 192
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_54:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_54._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_54._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 256
        - 2048
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  truediv_54:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_54._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_54._input_1_quantizer
        input_dtype: torch.float32
        input_shape: []
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  softmax_27:
    output_shape:
    - 1
    - 16
    - 2048
    - 2048
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: softmax_27._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  matmul_55:
    output_shape:
    - 1
    - 16
    - 2048
    - 256
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_55._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 2048
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: int8
      axis: 1
      dynamic: false
      etc_for_MCLab:
        torch_name: matmul_55._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 16
        - 2048
        - 256
        nbits: 8
        per_ch: true
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: minmax
        asymmetric: true
        do_quant: true
        do_zp_equalizing: true
  mul_137:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_137._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_137._input_1_quantizer
        input_dtype: int
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  truediv_55:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_55._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: truediv_55._input_1_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
  erf_27:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: erf_27._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_138:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_138._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_138._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_194:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_194._input_0_quantizer
        input_dtype: float
        input_shape:
        - 0
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: None
        calibrator_method: None
        asymmetric: false
        do_quant: false
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_194._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  mul_139:
    output_shape:
    - 1
    - 2048
    - 16384
    quant_desc_input_0:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_139._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: mul_139._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 16384
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_195:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_195._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_195._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  add_196:
    output_shape:
    - 1
    - 2048
    - 4096
    quant_desc_input_0:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_196._input_0_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
    quant_desc_input_1:
      dtype: bf16
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: add_196._input_1_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 4096
        nbits: 16
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
  output_0_quantize_node:
    output_shape:
    - 1
    - 2048
    - 50401
    quant_desc_input:
      dtype: fp32
      axis: null
      dynamic: false
      etc_for_MCLab:
        torch_name: output_0_quantize_node._input_quantizer
        input_dtype: torch.float32
        input_shape:
        - 1
        - 2048
        - 50401
        nbits: 32
        per_ch: false
        if_per_channel_scaling: false
        group_size: null
        unsigned: false
        calibrator_type: minmax
        calibrator_method: None
        asymmetric: true
        do_quant: true
        do_zp_equalizing: false
